{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB4eRmSXCrMO"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "#  \n",
        "  \n",
        "# Demystifying AI - Session 0\n",
        "## Programming Paradigms in AI: From Classical ML to Deep Learning\n",
        "\n",
        "\n",
        "### Pate Motter, PhD  \n",
        "\n",
        "AI Performance Engineer @ Google\n",
        "\n",
        "[LinkedIn](https://www.linkedin.com/in/patemotter/) | [GitHub](https://github.com/patemotter)\n",
        "\n",
        "---\n",
        "\n",
        "</div>\n",
        "\n",
        "## About This Notebook\n",
        "This notebook provides a practical comparison of three major programming paradigms in AI development:\n",
        "1. Traditional Programming\n",
        "2. Classical Machine Learning\n",
        "3. Deep Learning\n",
        "\n",
        "We'll implement spam detection using each approach, highlighting the strengths, weaknesses, and key differences between these paradigms.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzxmqfvbtoVp"
      },
      "source": [
        "## Getting Started\n",
        "This notebook provides an interactive exploration of different programming paradigms in AI. To run this notebook in Google Colab you will need:\n",
        "- A Google account to run this in Colab\n",
        "- About 60 minutes to go through the material\n",
        "\n",
        "NOTES:\n",
        "1. This colab is designed to run in the free tier of Google Colab.\n",
        "2. You are free to take this notebook and do whatever you want with it.\n",
        "\n",
        "Follow the instructions below to run this Colab:\n",
        "\n",
        "<details>\n",
        "<summary>1. Click Runtime -> Change runtime type</summary>\n",
        "\n",
        "![Screenshot](https://drive.google.com/uc?export=view&id=13tysKrMzwMkGRQo8qmll1-YvUeabQEh5)\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>2. Change selection to CPU</summary>\n",
        "\n",
        "For this notebook, we'll use the CPU runtime as we don't need GPU acceleration.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>3. Click Runtime -> Run all</summary>\n",
        "\n",
        "![Screenshot](https://drive.google.com/uc?export=view&id=1q0X-Rtzt3KgOnGPiM_uyGbA_kSFj4mlG)\n",
        "\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYILoSzlwYDq"
      },
      "source": [
        "## What You'll Learn in this Notebook\n",
        "This interactive notebook will teach you:\n",
        "- How different programming paradigms approach the same problem\n",
        "- When to use each approach\n",
        "- The evolution from rules to learning\n",
        "- Practical implementation differences\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2rjfeAzC3j6"
      },
      "source": [
        "# Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-Mxb3UtGYyh"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch scikit-learn nltk pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfElT6fg0KfT"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZYRtNDH2ctd"
      },
      "source": [
        "### Load Sample Data\n",
        "\n",
        "We'll use a sample dataset of spam and non-spam messages for our examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPq2kCze1tkG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data creation\n",
        "spam_texts = [\n",
        "    \"CONGRATULATIONS! You've WON $1,000,000! Click here to claim: www.claim-prize.com\",\n",
        "    \"FREE MONEY! Limited time offer! Visit now: www.free-cash.com!!!\",\n",
        "    \"You are the lucky winner of our daily prize! Send your details NOW!\",\n",
        "    \"Get RICH Quick! 100% Guaranteed! Click here: www.get-rich.com\",\n",
        "    \"URGENT: Your bank account is at risk! Verify now: www.secure-bank.com\",\n",
        "    \"Lose weight FAST with this miracle pill! Order now!\",\n",
        "    \"You've been selected for a FREE vacation! Claim here: www.free-trip.com\",\n",
        "    \"Make $$$ from home! Easy work, high pay. Apply now.\",\n",
        "    \"Your package has been delayed. Track its status: www.track-package.com\",\n",
        "    \"Your credit card has been charged. Call this number if it wasn't you.\",\n",
        "    \"Hot singles in your area! Chat now: www.dating-site.com\",\n",
        "    \"Pre-approved for a loan with 0% interest! Apply today.\",\n",
        "    \"Your lottery ticket is a winner! Claim your prize here.\",\n",
        "    \"Invest in this once-in-a-lifetime opportunity and become a millionaire!\",\n",
        "    \"Secret to eternal youth discovered! Learn more here.\",\n",
        "    \"Exclusive offer: 90% off on all designer brands!\",\n",
        "    \"Your account has been compromised. Please reset your password: www.account-reset.com\",\n",
        "    \"You've won a gift card! Redeem it now.\",\n",
        "    \"Double your money in 24 hours! Guaranteed returns.\",\n",
        "    \"Eliminate debt with this one simple trick!\"\n",
        "]\n",
        "\n",
        "ham_texts = [\n",
        "    \"Hi, can we meet at 3pm tomorrow to discuss the project?\",\n",
        "    \"Remember to pick up milk on your way home\",\n",
        "    \"The meeting has been rescheduled to next Monday\",\n",
        "    \"Great work on the presentation yesterday!\",\n",
        "    \"Don't forget to submit your report by Friday\",\n",
        "    \"Can you send me the meeting minutes from last week?\",\n",
        "    \"What time is the team lunch today?\",\n",
        "    \"I'll be out of the office next week. Please contact Sarah for urgent matters.\",\n",
        "    \"Have you seen the latest project proposal?\",\n",
        "    \"Let's grab coffee and catch up soon.\",\n",
        "    \"The deadline for the proposal is approaching. Please review the document.\",\n",
        "    \"Did you receive my email about the budget update?\",\n",
        "    \"Please confirm your attendance for the training session.\",\n",
        "    \"The client called and wants to schedule another meeting.\",\n",
        "    \"What's the status of the marketing campaign?\",\n",
        "    \"I've attached the revised contract. Please take a look.\",\n",
        "    \"Can you help me with this technical issue?\",\n",
        "    \"Reminder: Team building activity this Friday.\",\n",
        "    \"How's the new project going?\",\n",
        "    \"Just wanted to say thank you for your help.\"\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'text': spam_texts + ham_texts,\n",
        "    'true_label': ['spam'] * len(spam_texts) + ['ham'] * len(ham_texts)\n",
        "})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csLPOMxP3KPj"
      },
      "source": [
        "# 1. Traditional Programming\n",
        "\n",
        "## What You'll Learn in This Section\n",
        "- How rule-based systems work\n",
        "- Writing explicit logic for spam detection\n",
        "- Advantages and limitations of hard-coded rules\n",
        "- When this approach makes sense\n",
        "\n",
        "## What is Traditional Programming?\n",
        "In traditional programming, we explicitly write rules that define what spam looks like. For example:\n",
        "- Contains specific keywords\n",
        "- Uses all caps\n",
        "- Has many exclamation marks\n",
        "- Contains suspicious URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOXUCGjQJ40p"
      },
      "outputs": [],
      "source": [
        "def is_spam_traditional(text):\n",
        "    # Convert to lowercase for consistent checking\n",
        "    text = text.lower()\n",
        "\n",
        "    # Define spam indicators\n",
        "    spam_keywords = ['won', 'winner', 'cash', 'prize', 'money', 'click', 'free' 'urgent', 'now']\n",
        "    suspicious_patterns = {\n",
        "        \"exclamation_marks\": text.count('!') > 2,  # Too many exclamation marks\n",
        "        \"dollar_signs\": text.count('$') > 1,  # Multiple dollar signs\n",
        "        \"keywords_matched\": any(word in text for word in spam_keywords),  # Contains spam keywords\n",
        "        \"urls\": 'www.' in text or 'http' in text,  # Contains URLs\n",
        "    }\n",
        "\n",
        "    # If more than 2 patterns match, classify as spam\n",
        "    if sum(suspicious_patterns.values()) >= 2:\n",
        "      return 'spam'\n",
        "    else:\n",
        "      return 'ham'\n",
        "\n",
        "# Run the rules on all of the text examples\n",
        "data['traditional_label'] = data['text'].apply(is_spam_traditional)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, but now how do we measure the success of our new spam filter?\n",
        "\n",
        "We can use a few standard statistical methods to view this. Pandas can produce something called a \"Classification Report\" that provides some of these key insights."
      ],
      "metadata": {
        "id": "E4gBRj2R3L1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to read a classification report:\n",
        "---\n",
        "\n",
        "### Precision\n",
        "When our filter says an email is spam, how often is it actually spam?\n",
        "* High precision: Our filter is very picky and only flags emails as spam if it's very sure. You can trust its \"spam\" label.\n",
        "* Low precision: Our filter flags a lot of emails as spam, but many of them are actually legitimate.\n",
        "---\n",
        "\n",
        "### Recall\n",
        "Out of all the actual spam emails, how many does our filter successfully catch?\n",
        "* High recall: Our filter is very good at catching almost all the spam, even if it sometimes makes mistakes.\n",
        "* Low recall: Your filter misses a lot of spam, and those spam emails end up in your inbox.\n",
        "---\n",
        "\n",
        "### F1-Score:\n",
        "  \n",
        "A balanced average of precision and recall. It's useful when you want to consider both false positives and false negatives equally.\n",
        "\n",
        "---\n",
        "\n",
        "### Accuracy\n",
        "\n",
        "The overall percentage of emails that your filter classified correctly (both spam and ham).\n",
        "* Accuracy is a general measure of correctness, but it can be misleading if you have a lot more of one type of email than the other.\n",
        "\n",
        "---\n",
        "### Support\n",
        "* This simply tells you how many emails of each type (spam or ham) were in your test set.\n"
      ],
      "metadata": {
        "id": "IDhSxCsNKAup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Traditional programming classification report:\\n\")\n",
        "traditional_cr = classification_report(data['true_label'], data['traditional_label'], labels=[\"spam\", \"ham\"])\n",
        "print(traditional_cr)"
      ],
      "metadata": {
        "id": "EKD1mlI0FwVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what can we take away from these results?\n",
        "\n",
        "Spam:\n",
        "1. Precision=1.0: When the model predicts an email as \"spam,\" it is always correct.\n",
        "2. Recall=0.3: The model only identifies 30% of the actual spam emails correctly. 70% of spam emails are misclassified as ham.\n",
        "\n",
        "Ham:\n",
        "1. Precision=0.59: When the model predicts \"ham,\" it is correct only 59% of the time. 41% of the ham predictions are actually spam.\n",
        "2. Recall=1.00: The model correctly identifies all the actual ham emails.\n",
        "\n",
        "The filter is very good at identifying \"ham\" emails (perfect recall) but struggles with \"spam.\" It has a very high precision for \"spam\" (no false positives), but this comes at the cost of very low recall (missing most of the actual spam)."
      ],
      "metadata": {
        "id": "Zb10_mht3nvL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PjFuob9_QFt"
      },
      "source": [
        "## Analysis of Traditional Programming\n",
        "\n",
        "### Advantages\n",
        "- Simple to implement\n",
        "- No training data needed\n",
        "- Fast execution\n",
        "- Easy to modify rules\n",
        "- Completely transparent decision-making\n",
        "\n",
        "### Limitations\n",
        "- Cannot handle unseen patterns\n",
        "- Requires manual rule updates\n",
        "- Rules may conflict\n",
        "- Cannot learn from mistakes\n",
        "- Difficult to maintain as rules grow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGsF5r8_QFt"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. Classical Machine Learning\n",
        "\n",
        "## What You'll Learn in This Section\n",
        "\n",
        "*   How traditional ML approaches text classification\n",
        "*   Feature extraction techniques, including TF-IDF\n",
        "*   Training a basic classifier (Naive Bayes)\n",
        "*   Advantages over rule-based systems\n",
        "*   How TF-IDF and Naive Bayes work together for text classification\n",
        "\n",
        "## What is Classical Machine Learning?\n",
        "\n",
        "Instead of writing explicit rules to classify text (like in rule-based systems), classical machine learning takes a different approach:\n",
        "\n",
        "1.  **Convert text to numbers (features):** We transform text data into a numerical representation that the machine learning model can understand.\n",
        "2.  **Train a model on examples:** We provide the model with a dataset of labeled examples (e.g., spam and ham emails with their corresponding labels).\n",
        "3.  **Let the model find patterns:** The model learns the statistical relationships between the numerical features and the labels, effectively discovering patterns that distinguish between different classes of text.\n",
        "\n",
        "## Implementation\n",
        "\n",
        "We'll use a basic ML pipeline with:\n",
        "\n",
        "*   **TF-IDF vectorization for feature extraction:**  We'll use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into numerical vectors.\n",
        "*   **Naive Bayes classifier for prediction:** We'll employ a Naive Bayes algorithm to classify the text based on the learned patterns.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "**What it is:**\n",
        "TF-IDF helps to create a numerical representation of each email, where words that are indicative of spam (e.g., \"free,\" \"money,\" \"urgent\") will likely have higher TF-IDF weights in spam emails, while more general words will have lower weights.\n",
        "\n",
        "TF-IDF is a numerical statistic used to reflect how important a word is to a document in a collection of documents (a corpus). It's a way of representing text data as numerical vectors, which is essential for many machine learning algorithms.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "TF-IDF calculates a weight for each word in each document based on two factors:\n",
        "\n",
        "*   **Term Frequency (TF):** How often a word appears in a specific document. A higher TF suggests the word is more important to that document.\n",
        "\n",
        "    *   *Formula (one common variation):*  `TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)`\n",
        "\n",
        "*   **Inverse Document Frequency (IDF):** How common or rare a word is across the entire corpus. Words that appear in many documents get a lower IDF score, while words that appear in only a few documents get a higher IDF score. This helps to give more weight to distinctive words.\n",
        "\n",
        "    *   *Formula (one common variation):* `IDF(t) = log_e(Total number of documents / Number of documents with term t in it)`\n",
        "\n",
        "*   **TF-IDF Score:** The TF-IDF score for a word in a document is calculated by multiplying its TF and IDF scores.\n",
        "\n",
        "    *   *Formula:* `TF-IDF(t, d) = TF(t, d) * IDF(t)`\n",
        "\n",
        "**Why it's useful:**\n",
        "\n",
        "*   **Transforms text into numbers:** Machine learning models generally work with numerical data, not raw text. TF-IDF converts text into numerical vectors, where each dimension corresponds to a word in the vocabulary and the value represents the word's importance (TF-IDF weight).\n",
        "*   **Highlights important words:** TF-IDF gives higher weights to words that are frequent in a specific document but relatively rare in the overall corpus. This helps to identify words that are likely to be more relevant to the meaning of that document.\n",
        "*   **Reduces the impact of common words:** Common words like \"the,\" \"a,\" and \"is\" often appear in many documents and don't carry much specific meaning. IDF helps to downweight these words, preventing them from dominating the representation.\n"
      ],
      "metadata": {
        "id": "nzSqMjcm6kKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. It's called \"naive\" because it makes a simplifying assumption that all features (in this case, words) are independent of each other, which is often not true in reality but can still work surprisingly well in practice.\n",
        "\n",
        "**How it works (for text classification):**\n",
        "\n",
        "1.  **Training Phase:**\n",
        "    *   The algorithm calculates the prior probability of each class (e.g., the probability of an email being spam or ham based on the training data).\n",
        "    *   For each word in the vocabulary, it calculates the conditional probability of that word given each class (e.g., the probability of the word \"free\" appearing in a spam email, and the probability of it appearing in a ham email). These probabilities are often estimated using the frequency of the words in the training data.\n",
        "\n",
        "2.  **Prediction Phase:**\n",
        "    *   When a new email comes in, it's converted into a numerical vector using TF-IDF (or another method like `CountVectorizer`).\n",
        "    *   The algorithm then uses Bayes' theorem to calculate the posterior probability of each class given the words in the email (and their TF-IDF weights).\n",
        "    *   It classifies the email into the class with the highest posterior probability.\n",
        "\n",
        "**Bayes' Theorem (Simplified):**\n",
        "\n",
        "`P(Class | Words) = [P(Words | Class) * P(Class)] / P(Words)`\n",
        "\n",
        "*   `P(Class | Words)`: The probability that the email belongs to a specific class (spam or ham) given the words in the email.\n",
        "*   `P(Words | Class)`: The probability of observing those words given that the email is of a specific class (calculated during training).\n",
        "*   `P(Class)`: The prior probability of that class (calculated during training).\n",
        "*   `P(Words)`: The probability of observing those words (often ignored for comparison, as it's the same for all classes).\n",
        "\n",
        "**Why it's useful for text classification:**\n",
        "\n",
        "*   **Simple and Efficient:** Naive Bayes is relatively simple to implement and computationally efficient, especially for high-dimensional data like text.\n",
        "*   **Works Well with Text:** Despite the \"naive\" independence assumption, it often performs surprisingly well for text classification tasks.\n",
        "*   **Good with Limited Data:** It can perform reasonably well even with relatively small datasets, making it a good choice when you don't have a massive amount of training data."
      ],
      "metadata": {
        "id": "2iECDL7P6dZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How TF-IDF and Naive Bayes Work Together\n",
        "\n",
        "1.  **TF-IDF creates the input:** You use TF-IDF to transform your raw text data (emails) into numerical vectors. Each email is represented by a vector where each element corresponds to the TF-IDF weight of a word in the vocabulary.\n",
        "2.  **Naive Bayes uses the TF-IDF vectors:** The Naive Bayes classifier is trained on these TF-IDF vectors and their corresponding labels (spam or ham). It learns the probabilities of words (and their weights) given each class.\n",
        "3.  **Classification:** When a new email arrives, it's first converted into a TF-IDF vector, and then the Naive Bayes classifier uses the learned probabilities to predict the class (spam or ham) to which the email most likely belongs.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "*   TF-IDF provides a meaningful numerical representation of the text data.\n",
        "*   Naive Bayes uses these numerical representations to learn a probabilistic model for classifying text.\n",
        "\n",
        "By combining these two techniques, you can build a relatively simple yet effective text classification system, such as your spam filter."
      ],
      "metadata": {
        "id": "fY0d_5Gp6w0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vGWBCpZ_QFu"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class SpamClassifierML:\n",
        "    def __init__(self):\n",
        "        # Create a pipeline with vectorizer and classifier\n",
        "        self.pipeline = Pipeline([\n",
        "            ('vectorizer', TfidfVectorizer(max_features=1000)),\n",
        "            ('classifier', MultinomialNB())\n",
        "        ])\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        \"\"\"Train the classifier on the provided texts and labels\"\"\"\n",
        "        self.pipeline.fit(texts, labels)\n",
        "\n",
        "    def predict(self, texts):\n",
        "        \"\"\"Predict labels for the provided texts\"\"\"\n",
        "        return self.pipeline.predict(texts)\n",
        "\n",
        "# Create train/test split using the full dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['text'], data['true_label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the ML model\n",
        "ml_classifier = SpamClassifierML()\n",
        "ml_classifier.train(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "ml_predictions_test = ml_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "# Now, predict on the entire dataset for a complete evaluation\n",
        "ml_predictions_all = ml_classifier.predict(data['text'])\n",
        "data['ml_label'] = ml_predictions_all\n",
        "\n",
        "print(\"\\nMachine Learning Results (Full Dataset):\\n\")\n",
        "print(classification_report(data['true_label'], data['ml_label']))\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu8Xnd64_QFu"
      },
      "source": [
        "## Analysis of Machine Learning\n",
        "\n",
        "### Advantages\n",
        "- Learns from data\n",
        "- Can handle new patterns\n",
        "- Relatively simple to implement\n",
        "- Fast training and inference\n",
        "- Works well with limited data\n",
        "\n",
        "### Limitations\n",
        "- Requires good feature engineering\n",
        "- May miss complex patterns\n",
        "- Limited by feature design\n",
        "- Cannot handle very long-range dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky1u7_bd_QFu"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. Deep Learning\n",
        "\n",
        "## What You'll Learn in This Section\n",
        "\n",
        "*   The basics of deep learning for text classification\n",
        "*   How to build a neural network using PyTorch\n",
        "*   Key components of a deep learning model:\n",
        "    *   Neural network architecture\n",
        "    *   Loss function\n",
        "    *   Optimizer\n",
        "*   Training and evaluating a deep learning model\n",
        "*   Data augmentation techniques for text\n",
        "*   Why deep learning needs a lot of data\n",
        "\n",
        "## What is Deep Learning?\n",
        "\n",
        "Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to learn complex patterns from data. Unlike classical machine learning, where we often manually engineer features (like with TF-IDF), deep learning models can automatically learn hierarchical representations of data.\n",
        "\n",
        "**Neural Networks:**\n",
        "\n",
        "Neural networks are inspired by the structure of the human brain. They consist of interconnected nodes (neurons) organized in layers. Each connection has a weight, which is adjusted during training.\n",
        "\n",
        "*   **Input Layer:** Receives the input data (e.g., numerical representation of text).\n",
        "*   **Hidden Layers:** Multiple layers that perform computations on the input and learn increasingly complex features.\n",
        "*   **Output Layer:** Produces the final prediction (e.g., probability of spam or ham).\n",
        "\n",
        "## Data Augmentation for Text\n",
        "\n",
        "Since our dataset is small, we'll use data augmentation to artificially increase its size and improve the model's ability to generalize. Common text augmentation techniques include:\n",
        "\n",
        "*   **Synonym Replacement:** Replacing words with their synonyms.\n",
        "*   **Random Deletion:** Randomly removing words from a sentence.\n",
        "*   **Random Swap:** Randomly swapping the positions of words in a sentence.\n",
        "*   **Random Insertion:** Randomly inserting new words (often synonyms of existing words) into a sentence.\n",
        "\n",
        "## Implementation\n",
        "\n",
        "Our deep learning model will use:\n",
        "\n",
        "*   **PyTorch:** A popular deep learning framework.\n",
        "*   **`CountVectorizer`:** To convert text into numerical vectors (simpler than TF-IDF for this example).\n",
        "*   **A simple feedforward neural network:** With a few hidden layers.\n",
        "*   **Data augmentation:** To increase the size of our training set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Download NLTK data (if you haven't already)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# --- Data Augmentation Functions ---\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"Get synonyms for a word using WordNet.\"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for l in syn.lemmas():\n",
        "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "            synonyms.add(synonym)\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "def random_deletion(words, p=0.2):\n",
        "    \"\"\"Randomly delete words from a sentence with probability p.\"\"\"\n",
        "    if len(words) == 1:\n",
        "        return words\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.uniform(0, 1) > p:\n",
        "            new_words.append(word)\n",
        "    if len(new_words) == 0:\n",
        "        return [random.choice(words)]\n",
        "    return new_words\n",
        "\n",
        "def random_swap(words, n=2):\n",
        "    \"\"\"Randomly swap n pairs of words in a sentence.\"\"\"\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        if len(words) < 2:\n",
        "            break\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
        "    return new_words\n",
        "\n",
        "def random_insertion(words, n=1):\n",
        "    \"\"\"Randomly insert n words into a sentence.\"\"\"\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        if not words:\n",
        "            continue\n",
        "        random_word = random.choice(words)\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if synonyms:\n",
        "            new_word = random.choice(synonyms)\n",
        "            insert_idx = random.randint(0, len(new_words))\n",
        "            new_words.insert(insert_idx, new_word)\n",
        "    return new_words\n",
        "\n",
        "def augment_text(text, p_del=0.2, n_swap=1, n_ins=1):\n",
        "    \"\"\"Apply augmentations to a text.\"\"\"\n",
        "    words = text.split()\n",
        "    augmented_texts = [text]\n",
        "    augmented_texts.append(\" \".join(random_deletion(words, p=p_del)))\n",
        "    augmented_texts.append(\" \".join(random_swap(words, n=n_swap)))\n",
        "    augmented_texts.append(\" \".join(random_insertion(words, n=n_ins)))\n",
        "    return augmented_texts\n",
        "\n",
        "# --- Your Original Data ---\n",
        "spam_texts = [\n",
        "    \"CONGRATULATIONS! You've WON $1,000,000! Click here to claim: www.claim-prize.com\",\n",
        "    \"FREE MONEY! Limited time offer! Visit now: www.free-cash.com!!!\",\n",
        "    \"You are the lucky winner of our daily prize! Send your details NOW!\",\n",
        "    \"Get RICH Quick! 100% Guaranteed! Click here: www.get-rich.com\",\n",
        "    \"URGENT: Your bank account is at risk! Verify now: www.secure-bank.com\",\n",
        "    \"Lose weight FAST with this miracle pill! Order now!\",\n",
        "    \"You've been selected for a FREE vacation! Claim here: www.free-trip.com\",\n",
        "    \"Make $$$ from home! Easy work, high pay. Apply now.\",\n",
        "    \"Your package has been delayed. Track its status: www.track-package.com\",\n",
        "    \"Your credit card has been charged. Call this number if it wasn't you.\",\n",
        "    \"Hot singles in your area! Chat now: www.dating-site.com\",\n",
        "    \"Pre-approved for a loan with 0% interest! Apply today.\",\n",
        "    \"Your lottery ticket is a winner! Claim your prize here.\",\n",
        "    \"Invest in this once-in-a-lifetime opportunity and become a millionaire!\",\n",
        "    \"Secret to eternal youth discovered! Learn more here.\",\n",
        "    \"Exclusive offer: 90% off on all designer brands!\",\n",
        "    \"Your account has been compromised. Please reset your password: www.account-reset.com\",\n",
        "    \"You've won a gift card! Redeem it now.\",\n",
        "    \"Double your money in 24 hours! Guaranteed returns.\",\n",
        "    \"Eliminate debt with this one simple trick!\"\n",
        "]\n",
        "\n",
        "ham_texts = [\n",
        "    \"Hi, can we meet at 3pm tomorrow to discuss the project?\",\n",
        "    \"Remember to pick up milk on your way home\",\n",
        "    \"The meeting has been rescheduled to next Monday\",\n",
        "    \"Great work on the presentation yesterday!\",\n",
        "    \"Don't forget to submit your report by Friday\",\n",
        "    \"Can you send me the meeting minutes from last week?\",\n",
        "    \"What time is the team lunch today?\",\n",
        "    \"I'll be out of the office next week. Please contact Sarah for urgent matters.\",\n",
        "    \"Have you seen the latest project proposal?\",\n",
        "    \"Let's grab coffee and catch up soon.\",\n",
        "    \"The deadline for the proposal is approaching. Please review the document.\",\n",
        "    \"Did you receive my email about the budget update?\",\n",
        "    \"Please confirm your attendance for the training session.\",\n",
        "    \"The client called and wants to schedule another meeting.\",\n",
        "    \"What's the status of the marketing campaign?\",\n",
        "    \"I've attached the revised contract. Please take a look.\",\n",
        "    \"Can you help me with this technical issue?\",\n",
        "    \"Reminder: Team building activity this Friday.\",\n",
        "    \"How's the new project going?\",\n",
        "    \"Just wanted to say thank you for your help.\"\n",
        "]\n",
        "\n",
        "# --- Create Augmented DataFrame ---\n",
        "data = []\n",
        "\n",
        "for text in spam_texts:\n",
        "    augmented_texts = augment_text(text)\n",
        "    for aug_text in augmented_texts:\n",
        "        data.append({'text': aug_text, 'label': 'spam'})\n",
        "\n",
        "for text in ham_texts:\n",
        "    augmented_texts = augment_text(text)\n",
        "    for aug_text in augmented_texts:\n",
        "        data.append({'text': aug_text, 'label': 'ham'})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# --- Split Data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Text Dataset Class ---\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vectorizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vectorizer = vectorizer\n",
        "        self.label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx] if isinstance(self.texts, pd.Series) else self.texts[idx]\n",
        "        label = self.labels.iloc[idx] if isinstance(self.labels, pd.Series) else self.labels[idx]\n",
        "        if isinstance(label, str):\n",
        "            label = self.label_map[label]\n",
        "        vector = torch.tensor(self.vectorizer.transform([text]).toarray()[0])\n",
        "        return vector.float(), torch.tensor(label).float()\n",
        "\n",
        "# --- Spam Classifier Model ---\n",
        "class SpamClassifierDL(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 64)\n",
        "        self.layer2 = nn.Linear(64, 16)\n",
        "        self.layer3 = nn.Linear(16, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "# --- Initialize Vectorizer and Create Datasets ---\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "vectorizer.fit(X_train)\n",
        "vocab_size = len(vectorizer.vocabulary_)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train, vectorizer)\n",
        "test_dataset = TextDataset(X_test, y_test, vectorizer)\n",
        "\n",
        "# --- Create Data Loaders ---\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# --- Initialize Model, Criterion, and Optimizer ---\n",
        "model = SpamClassifierDL(input_dim=vocab_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# --- Train the Model ---\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() >= 0.5).float()\n",
        "        predictions.extend(predicted.tolist())\n",
        "        actuals.extend(labels.tolist())\n",
        "\n",
        "label_map = {i: label for i, label in enumerate(set(y_train))}\n",
        "predictions = [label_map[int(p)] for p in predictions]\n",
        "actuals = [label_map[int(a)] for a in actuals]\n",
        "\n",
        "print(\"\\nDeep Learning Results (Test Set):\\n\")\n",
        "print(classification_report(actuals, predictions))\n",
        "\n",
        "# --- Evaluate on Full Dataset ---\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_actuals = []\n",
        "\n",
        "full_dataset = TextDataset(df['text'], df['label'], vectorizer)\n",
        "full_loader = DataLoader(full_dataset, batch_size=32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in full_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() >= 0.5).float()\n",
        "        all_predictions.extend(predicted.tolist())\n",
        "        all_actuals.extend(labels.tolist())\n",
        "\n",
        "all_predictions = [label_map[int(p)] for p in all_predictions]\n",
        "all_actuals = [label_map[int(a)] for a in all_actuals]\n",
        "\n",
        "print(\"\\nDeep Learning Results (Full Dataset):\\n\")\n",
        "print(classification_report(all_actuals, all_predictions))"
      ],
      "metadata": {
        "id": "Zt9vmXoiUgDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did it gang! We built the perfect email filter.\n",
        "\n",
        "1.00s in every category, what could possibli go wrong?\n",
        "\n",
        "Actually a lot, it's called overfitting. We used a very small amount of data to train our model and it has basically memorized this set of data completely. We need to find some new data to actually test our model.\n"
      ],
      "metadata": {
        "id": "06rIAFjdWyVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_data = pd.DataFrame({\n",
        "    'text': [\n",
        "        \"The mitochondria is the powerhouse of the cell. Eukaryotic organisms leverage oxidative phosphorylation for ATP synthesis.\",  # Ham - scientific, very specific jargon\n",
        "        \"Quasar 3C 273 is an active galactic nucleus exhibiting relativistic jets and strong radio emissions.\",  # Ham - astrophysics, highly technical\n",
        "        \"Epistemological considerations in qualitative research methodologies require reflexivity and bracketing of researcher bias.\",  # Ham - academic, philosophical\n",
        "        \"My dude, that party last night was totally lit 🔥! We should do it again sometime.\",  # Ham - very informal, slang, emoji\n",
        "        \"Has anyone seen my keys? I think I left them somewhere in the house. 🤔\",  # Ham - common everyday question, emoji\n",
        "        \"Pneumonoultramicroscopicsilicovolcanoconiosis is a lung disease caused by the inhalation of very fine silica dust.\", # Ham - extremely long, technical word\n",
        "        \"The quick brown fox jumps over the lazy dog. 1234567890 !@#$%^&*()\",  # Ham - pangram, numbers, symbols\n",
        "        \"To be or not to be, that is the question. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune...\", # Ham - famous quote, Shakespearean English\n",
        "        \"In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell...\", # Ham - famous quote, fantasy literature\n",
        "        \"Just finished a great workout at the gym 💪. Feeling energized! #fitness #healthyliving\",  # Ham - social media style, hashtag\n",
        "        \"OMG! Did you hear about the latest celebrity gossip? 😲 Spilling the tea ☕ on my blog: www.gossip-central.com\",  # Spam - informal, internet slang, clickbaity\n",
        "        \"SUPER EXCLUSIVE!!! ONE-TIME OFFER!!! Get a FREE sample of our revolutionary new cryptocurrency! www.definitely-not-a-pyramid-scheme.com\",  # Spam - different topic, very spammy\n",
        "        \"You are hereby cordially invited to an evening of intrigue and mystery. RSVP at www.this-sounds-suspicious.com\",  # Spam - different style, formal but unusual\n",
        "        \"Participate in our survey for a chance to win an all-expenses-paid trip to a remote, undisclosed location! www.enter-at-your-own-risk.com\",  # Spam - vague, potentially dangerous\n",
        "        \"This ancient herbal remedy can cure any ailment! Limited supply, order now! www.snake-oil-emporium.com\",  # Spam - implausible claim, different topic\n",
        "        \"Foreign dignitary seeks assistance in transferring large sum of money. Generous compensation offered. Contact: www.not-a-scam-at-all.com\", # Spam - a twist on the classic Nigerian prince scam\n",
        "        \"BREAKING NEWS: Evidence of extraterrestrial life discovered! Read the full story here: www.definitely-not-fake-news.com\", # Spam - outrageous claim\n",
        "        \"Your social security number has been flagged for suspicious activity. Call this number immediately to avoid legal action.\" # Spam - no link, different tactic, still spammy\n",
        "\n",
        "    ],\n",
        "    'label': ['ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam']\n",
        "})\n",
        "\n",
        "new_test_dataset = TextDataset(new_test_data['text'], new_test_data['label'], vectorizer)\n",
        "new_test_loader = DataLoader(new_test_dataset, batch_size=32)\n",
        "\n",
        "model.eval()\n",
        "new_predictions = []\n",
        "new_actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in new_test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() >= 0.5).float()\n",
        "        new_predictions.extend(predicted.tolist())\n",
        "        new_actuals.extend(labels.tolist())\n",
        "\n",
        "label_map = {i: label for i, label in enumerate(set(new_test_data['label']))}\n",
        "new_predictions = [label_map[int(p)] for p in new_predictions]\n",
        "new_actuals = [label_map[int(a)] for a in new_actuals]\n",
        "\n",
        "print(\"\\nDeep Learning Results (New Test Set):\\n\")\n",
        "print(classification_report(new_actuals, new_predictions))\n",
        "new_test_data[\"dl_label\"] = new_predictions\n",
        "new_test_data"
      ],
      "metadata": {
        "id": "A-70PmoWU3Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBZfYL0S_QFu"
      },
      "source": [
        "## When to Use Each Approach\n",
        "\n",
        "### Traditional Programming\n",
        "✅ **Best For**:\n",
        "- Simple, rule-based decisions\n",
        "- Need for complete transparency\n",
        "- Limited, well-defined patterns\n",
        "- No training data available\n",
        "- Quick prototyping\n",
        "\n",
        "### Classical Machine Learning\n",
        "✅ **Best For**:\n",
        "- Moderate amounts of data\n",
        "- Clear feature patterns\n",
        "- Need for balance of performance and simplicity\n",
        "- Resource constraints\n",
        "- Well-understood problem domain\n",
        "\n",
        "### Deep Learning\n",
        "✅ **Best For**:\n",
        "- Large datasets available\n",
        "- Complex patterns\n",
        "- Sequential or hierarchical data\n",
        "- High performance requirements\n",
        "- Resource availability\n",
        "- Need for state-of-the-art accuracy\n",
        "\n",
        "## Decision Framework\n",
        "\n",
        "When choosing between these approaches, consider:\n",
        "\n",
        "1. **Data Availability**\n",
        "   - No data → Traditional Programming\n",
        "   - Small dataset → Classical ML\n",
        "   - Large dataset → Deep Learning\n",
        "\n",
        "2. **Problem Complexity**\n",
        "   - Simple rules exist → Traditional Programming\n",
        "   - Clear features exist → Classical ML\n",
        "   - Complex patterns → Deep Learning\n",
        "\n",
        "3. **Resource Constraints**\n",
        "   - Limited computing power → Traditional Programming\n",
        "   - Moderate resources → Classical ML\n",
        "   - GPU available → Deep Learning\n",
        "\n",
        "4. **Maintenance Requirements**\n",
        "   - Frequent rule updates → Consider ML/DL\n",
        "   - Need for transparency → Traditional/Classical ML\n",
        "   - Automated learning needed → ML/DL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Y6uV0o_QFu"
      },
      "source": [
        "# Glossary\n",
        "\n",
        "## Traditional Programming Terms\n",
        "- **Rule-Based System**: Program that uses manually defined rules to make decisions\n",
        "- **Boolean Logic**: True/false conditions used in rules\n",
        "- **Control Flow**: How program decisions are made\n",
        "- **Deterministic**: Same input always produces same output\n",
        "- **Pattern Matching**: Finding specific text patterns using rules\n",
        "\n",
        "## Machine Learning Terms\n",
        "- **Feature**: Numerical representation of data\n",
        "- **Feature Engineering**: Process of creating features from raw data\n",
        "- **Training**: Process of learning from examples\n",
        "- **Classification**: Assigning categories to inputs\n",
        "- **Supervised Learning**: Learning from labeled examples\n",
        "\n",
        "## Deep Learning Terms\n",
        "- **Neural Network**: Computing system inspired by biological brains\n",
        "- **Layer**: Processing level in neural network\n",
        "- **Activation Function**: Non-linear function applied to layer outputs\n",
        "- **Batch**: Group of examples processed together\n",
        "*   **Weights and Biases:**  The connections between neurons have associated weights, and each neuron has a bias. These are the parameters that the model learns during training.\n",
        "*   **Activation Functions:**  Non-linear functions (like ReLU - Rectified Linear Unit) applied to the output of each neuron, introducing non-linearity into the model and enabling it to learn complex relationships.\n",
        "*   **Forward Pass:** The process of feeding input data through the network, performing calculations at each layer, and producing an output.\n",
        "*   **Loss Function:** A function that measures the difference between the model's predictions and the actual labels. The goal of training is to minimize this loss. (e.g. Binary Cross-Entropy Loss is often used for binary classification)\n",
        "*   **Optimizer:** An algorithm that adjusts the model's weights and biases to minimize the loss function. (e.g. Adam is a popular optimization algorithm)\n",
        "*   **Backpropagation:** The process of calculating the gradients of the loss function with respect to the model's weights and biases, used by the optimizer to update the parameters.\n",
        "*   **Epoch:** One complete pass through the entire training dataset.\n",
        "*   **Batch Size:** The number of training examples processed in one forward/backward pass.\n",
        "*   **Learning Rate:** A hyperparameter that controls how much the model's weights are adjusted in each update step\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- [scikit-learn Documentation](https://scikit-learn.org/)\n",
        "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
        "- [Machine Learning Mastery](https://machinelearningmastery.com/)\n",
        "\n",
        "---\n",
        "\n",
        "# License Information\n",
        "\n",
        "<details>\n",
        "<summary>License Information</summary>\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2024\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlfX1NId7ybZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}